# Chapter 8: Results Processing & Visualization

Welcome to the final chapter! In [Chapter 7: Simulation Runner & Parallelization](07_simulation_runner___parallelization_.qmd), we saw how the simulation is run multiple times (replications) to account for randomness, and how all the raw data from these runs is gathered into a single big file, usually called `data/run_results.csv`.

But what do we *do* with this big file? It contains thousands, maybe millions, of rows detailing every little event that happened in every simulation run. Just looking at the raw data won't tell us much about overall performance or answer questions like "Did adding that extra car actually help?"

**What Problem Does This Solve? The Need for Insights**

Imagine you're a manager who just received a massive spreadsheet listing every single sale made last year â€“ every item, every customer, every timestamp. Useful data, perhaps, but overwhelming! What you really need is a report summarizing:

*   Total sales this year vs. last year.
*   Average sale value.
*   Busiest sales hours.
*   A chart showing monthly sales trends.

Our simulation output (`run_results.csv`) is like that raw sales spreadsheet. The **Results Processing & Visualization** component acts like the **analytics department**. It takes this raw simulation log data and transforms it into meaningful insights. It calculates key performance indicators (KPIs), creates charts, and compares the simulation's output to historical data, presenting everything clearly in the [Web Application Interface (Streamlit)](01_web_application_interface__streamlit__.qmd).

**From Raw Data to Clear Results: The Workflow**

This component performs several key tasks, mostly handled by Python scripts found in the `visualisation/` folder:

1.  **Load Data:** It reads the combined results file (`data/run_results.csv`) generated by the [Simulation Runner & Parallelization](07_simulation_runner___parallelization_.qmd). It also loads relevant historical data from files typically stored in the `historical_data/` folder.
2.  **Process & Calculate:** It cleans and structures the data. Then, it performs calculations using libraries like `pandas` to determine important metrics (KPIs):
    *   **Resource Utilization:** What percentage of their available time were helicopters and cars busy? (Uses functions in `visualisation/_utilisation_result_calculation.py`)
    *   **Job Counts:** How many jobs were handled per month, per hour, or per day? How many were potentially missed? (Uses functions in `visualisation/_job_count_calculation.py`)
    *   **Activity Timings:** How long did key activities like mobilisation or time on scene take, on average? (Uses functions in `visualisation/_job_time_calcs.py`)
    *   **Process Flow:** Analyzing the sequence of events. (Uses functions in `visualisation/_process_analytics.py`)
3.  **Compare:** It compares the calculated metrics from the simulation against the loaded historical data. For example, comparing the simulated average monthly jobs to the actual average monthly jobs from past years.
4.  **Generate Outputs:** It creates:
    *   **KPI values:** Single numbers representing key metrics (e.g., average H70 utilisation = 25.3%).
    *   **Tables:** Structured summaries of data (e.g., a table showing utilisation for each resource).
    *   **Plots:** Visual charts (e.g., line charts, bar charts, box plots) created using libraries like `plotly`.
5.  **Display:** These generated KPIs, tables, and plots are then passed back to the Streamlit application (`app/model.py`), which uses functions like `st.metric`, `st.dataframe`, and `st.plotly_chart` to display them neatly in the web interface tabs.

**Example 1: Calculating a KPI (Resource Utilization)**

Let's say we want to calculate the average utilization for the H70 helicopter across all simulation runs.

*   **Input:** `data/run_results.csv` (need columns like `run_number`, `callsign`, `event_type`, `timestamp_dt`) and data on H70's scheduled availability (from [Resource Management (Availability & Allocation)](05_resource_management__availability___allocation__.qmd)).
*   **Logic (Simplified):**
    1.  Filter `run_results.csv` to get only the 'resource_use' and 'resource_use_end' events for 'H70'.
    2.  For each job H70 attended in each run, calculate the time duration between 'resource_use' and 'resource_use_end'.
    3.  Sum these durations for H70 within each `run_number`. This gives the total time H70 was busy in each run.
    4.  Determine the *total time H70 was available* during the simulation (considering its rota and any maintenance, calculated by helper functions perhaps using `visualisation/_vehicle_calculation.py`).
    5.  For each run, divide the total busy time by the total available time to get the utilisation percentage for that run.
    6.  Calculate the average of these utilisation percentages across all runs.
*   **Code:** The actual calculation happens inside functions within `visualisation/_utilisation_result_calculation.py`. A simplified conceptual snippet might look like:

    ```python
    # In visualisation/_utilisation_result_calculation.py (Conceptual)
    import pandas as pd
    # Assume 'resource_use_wide' DataFrame has columns:
    # 'run_number', 'callsign', 'resource_use_duration' (in minutes)
    # Assume 'total_avail_minutes' DataFrame has columns:
    # 'callsign', 'total_available_minutes_in_sim'

    def calculate_average_utilisation(resource_use_wide, total_avail_minutes):
        # Sum busy time per run and callsign
        utilisation_per_run = resource_use_wide.groupby(['run_number', 'callsign'])['resource_use_duration'].sum().reset_index()

        # Merge with available time
        utilisation_per_run = utilisation_per_run.merge(total_avail_minutes, on='callsign')

        # Calculate percentage utilisation for each run
        utilisation_per_run['perc_time_in_use'] = (
            utilisation_per_run['resource_use_duration'] /
            utilisation_per_run['total_available_minutes_in_sim']
        )

        # Calculate the average across all runs for each callsign
        average_utilisation = utilisation_per_run.groupby('callsign')['perc_time_in_use'].mean().reset_index()

        # Format as percentage string for display
        average_utilisation['PRINT_perc'] = average_utilisation['perc_time_in_use'].apply(lambda x: f"{x:.1%}")
        return average_utilisation # Return a DataFrame with results per callsign
    ```
*   **Display:** The Streamlit app (`app/model.py`) calls this calculation function and then uses `st.metric` to show the result:

    ```python
    # In app/model.py (Simplified)
    import streamlit as st
    import visualisation._utilisation_result_calculation as util_calcs

    # ... (simulation runs and results_all_runs DataFrame is loaded) ...

    # Call the function to calculate utilisation
    # (This function internally processes results_all_runs)
    resource_use_wide, utilisation_df_overall, _, _ = util_calcs.make_utilisation_model_dataframe(...)

    # Get the specific value for H70
    h70_util_fig = utilisation_df_overall[utilisation_df_overall['callsign']=='H70']['PRINT_perc'].values[0]

    # Display it using Streamlit's metric widget
    st.metric("Average Simulated H70 Utilisation", h70_util_fig)
    ```

**Example 2: Generating a Visualization (Monthly Jobs Plot)**

Let's create a plot showing the average number of jobs per month in the simulation compared to historical data.

*   **Input:** `data/run_results.csv` (need `run_number`, `P_ID`, `timestamp_dt`, `time_type`) and a historical data file like `historical_data/historical_monthly_totals_all_calls.csv`.
*   **Logic (Simplified):**
    1.  Filter `run_results.csv` for 'arrival' events (each arrival represents a potential job).
    2.  Extract the month and year from the `timestamp_dt` for each arrival.
    3.  Group the data by `run_number` and `month`, then count the number of arrivals (`P_ID`s) in each group. This gives the monthly job count for each run.
    4.  For each month, calculate the average count across all runs. Also, calculate measures of variation (like min/max or percentiles) across the runs for that month.
    5.  Load the historical monthly job counts from the CSV file.
    6.  Use `plotly` to generate a line chart:
        *   One line for the average simulated monthly count.
        *   A shaded area around the average line representing the variation (e.g., min to max) across runs.
        *   Another line or shaded area representing the historical data range.
*   **Code:** Functions in `visualisation/_job_count_calculation.py` handle this.

    ```python
    # In visualisation/_job_count_calculation.py (Conceptual)
    import pandas as pd
    import plotly.express as px
    import plotly.graph_objects as go

    def plot_monthly_calls(call_df, historical_monthly_job_data_path, ...):
        # Assume 'call_df' has arrival data per patient/run

        # Extract month
        call_df['timestamp_dt'] = pd.to_datetime(call_df['timestamp_dt'])
        call_df['month_start'] = call_df['timestamp_dt'].dt.to_period('M').dt.to_timestamp()

        # Calculate monthly calls per run
        call_counts_monthly = call_df.groupby(['run_number', 'month_start'])['P_ID'].count().reset_index()
        # ... (filter out partial start/end months) ...

        # Calculate average and variation (min/max) across runs
        summary = call_counts_monthly.groupby('month_start')['monthly_calls'].agg(['mean', 'min', 'max']).reset_index()

        # Load historical data
        historical_data = pd.read_csv(historical_monthly_job_data_path)
        # ... (process historical data similarly) ...

        # Create plot using Plotly
        fig = go.Figure()
        # Add shaded region for simulation range (min to max)
        fig.add_traces(...) # Min/Max lines with fill='tonexty'
        # Add line for simulation average
        fig.add_trace(go.Scatter(x=summary['month_start'], y=summary['mean'], name='Simulated Average', ...))
        # Add line/region for historical data
        fig.add_traces(...) # Historical data

        fig.update_layout(title="Monthly Calls: Simulation vs Historical", ...)
        return fig # Return the Plotly figure object
    ```
*   **Display:** The Streamlit app (`app/model.py`) calls this plotting function and displays the generated figure.

    ```python
    # In app/model.py (Simplified)
    import streamlit as st
    import visualisation._job_count_calculation as job_calcs

    # ... (simulation runs and results_all_runs DataFrame is loaded) ...
    # Prepare the 'call_df' needed by the plotting function
    call_df = job_calcs.make_job_count_df(path="data/run_results.csv", ...)

    # Call the function to generate the plot
    fig_monthly_calls = job_calcs.plot_monthly_calls(
        call_df,
        historical_monthly_job_data_path="historical_data/historical_monthly_totals_all_calls.csv",
        # ... other plot options ...
    )

    # Display the figure using Streamlit
    st.plotly_chart(fig_monthly_calls)
    ```

**Under the Hood: The Analysis Pipeline**

When you click "Run Simulation" and it finishes, a sequence of processing steps leads to the results you see:

```{mermaid}
sequenceDiagram
    participant App as Streamlit App (app/model.py)
    participant Collator as Result Collator (Ch 7)
    participant VisUtil as Utilisation Calcs (visualisation/*)
    participant VisJobs as Job Count Calcs (visualisation/*)
    participant Plotly as Plotting Library

    Note over App, Collator: Simulation Finished
    App->>Collator: collateRunResults()
    Collator-->>App: run_results.csv ready
    Note over App: Load run_results.csv into DataFrame

    App->>VisUtil: Calculate Utilisation(results_df)
    VisUtil->>VisUtil: Load historical util data (CSV)
    VisUtil->>VisUtil: Process data (Pandas: filter, group, calc durations)
    VisUtil-->>App: Return Utilisation KPIs & Tables

    App->>VisJobs: Generate Monthly Jobs Plot(results_df)
    VisJobs->>VisJobs: Load historical job counts (CSV)
    VisJobs->>VisJobs: Process data (Pandas: filter, group, count, average)
    VisJobs->>Plotly: Create Plotly figure with sim & historical data
    Plotly-->>VisJobs: Return Figure Object
    VisJobs-->>App: Return Figure Object

    Note over App: Display results in tabs
    App->>App: Display KPIs (st.metric(utilisation_kpi))
    App->>App: Display Plots (st.plotly_chart(jobs_figure))
```

Essentially, the scripts in the `visualisation/` folder act as specialized calculators and chart generators. They take the raw simulation log, perform specific analyses using `pandas`, often incorporate historical data for context, and produce outputs (numbers, tables, `plotly` figures) that the main Streamlit application (`app/model.py`) can then easily display.

**Conclusion**

The Results Processing & Visualization component is the crucial final step that transforms the raw output of our complex simulation into understandable information. By calculating key metrics like resource utilization and job counts, comparing them against historical benchmarks, and generating clear plots and tables, it allows users to effectively interpret the simulation's findings directly within the [Web Application Interface (Streamlit)](01_web_application_interface__streamlit__.qmd). This enables informed decision-making based on the simulation scenarios tested.

This concludes our tutorial series on the DAA_DES project. We hope this journey through the web interface, the simulation engine, the core entities (patients and resources), resource management, stochastic modeling, the simulation runner, and finally results processing has given you a clear understanding of how this powerful tool works!```

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)
